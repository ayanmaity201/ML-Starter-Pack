{
  "cells": [
    {
      "metadata": {
        "_uuid": "64f212ff468addb9e9ebcdaa2bca29533fbb72bb"
      },
      "cell_type": "markdown",
      "source": "# Pytorch Convolution Neural Network "
    },
    {
      "metadata": {
        "_uuid": "dbaf1668400511d391071eddcc552dc65c2c0a42"
      },
      "cell_type": "markdown",
      "source": "In this notebook I am going to build a convolution neural network using torch.nn module. We are going to train the CNN on MNIST data set . "
    },
    {
      "metadata": {
        "_cell_guid": "b98bf7d9-7676-49df-b59f-05466d666c16",
        "_uuid": "991df7918a2b37a1e9a0fd183f5e393daac82772",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "test.csv\ntrain.csv\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "b4575dbbf463237aea79241ceeaaa338e8664b6f"
      },
      "cell_type": "markdown",
      "source": "Here first we have to import several torch libraries like torch.nn , torch.data.utils, torchvision etc."
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "f5d0badcfc1c82178f0572ef370b5ee2abd49a6c"
      },
      "cell_type": "code",
      "source": "import torch\nimport torch.nn as nn\nimport torch.utils.data as data\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "04a0ea2a0c392cecd53fd700974afa688b942718"
      },
      "cell_type": "markdown",
      "source": "A very important part in building any deep NN in pytorch is first building a class conating the data . This data.datatset class wil contain  init() , getitem() , len() . After defining the class we create an object of that class . Then call the dataloader on the data with    batch_size = 50 .  "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b9ad66193bad4ce723b278afeb7e7ed40ec5dfa7"
      },
      "cell_type": "code",
      "source": "class Mnist(data.Dataset):\n    def __init__(self):\n        train_X = pd.read_csv(\"../input/train.csv\")\n        train_Y = train_X.label.as_matrix().tolist()\n        train_X = train_X.drop(\"label\",axis=1).as_matrix().reshape(42000,1,28,28)\n        self.datalist = train_X\n        self.labellist = train_Y\n\n\n    def __getitem__(self, index):\n        return torch.Tensor(self.datalist[index].astype(float)), self.labellist[index]\n\n    def __len__(self):\n        return self.datalist.shape[0]\n\ntrain_data = Mnist()\ntrain_loader = torch.utils.data.DataLoader(dataset=train_data,\n                                           batch_size=50, \n                                           shuffle=True,\n                                           num_workers=2)",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2545f81d9311997b9903db0f32845ded98164698"
      },
      "cell_type": "markdown",
      "source": "Now we create the nn.module class CNN . It must contain two methods init ( ) and forward ( ) . In init ( ) we define the sequential steps of the CNN model using Conv2d , ReLU , MaxPool2d etc. In forward method we do the forward propagation part . "
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "e1368d4e9170be21d9ef871888cdf4ceb486ea87"
      },
      "cell_type": "code",
      "source": "class CNN(nn.Module):\n    def __init__(self):\n        super(CNN,self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(1, 20, kernel_size=5),\n            nn.BatchNorm2d(20),\n            nn.ReLU(),\n            nn.MaxPool2d(2))\n        self.fc = nn.Linear(12*12*20, 10)\n\n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = out.view(out.size(0),-1)\n        out = self.fc(out)\n        return out\n    \ncnn = CNN()",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "89d979a8b4edbe06ad09e6bf28f6a3bae0606d2c"
      },
      "cell_type": "markdown",
      "source": "After craeting an instance cnn of class CNN now we define the loss function and the optimizer technique , which is SGD here with learning rate = .004"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "00bd7664bc900c1b32ed8ec8aa9f95fdd621a480"
      },
      "cell_type": "code",
      "source": "loss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(cnn.parameters(), lr=0.0004)",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2a034f9fbdfba7c42dd97dd9b6fbc2d9c27941e8"
      },
      "cell_type": "markdown",
      "source": "Here we start the actual CNN implemetation through iterating over allthe examples . Here we are only running 3 iterations and in each epoch we first call cnn on each image then we do the backward propagation and update the parameters . "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1c0afdb80c986b65eaa1ae4f3574d465085111dc"
      },
      "cell_type": "code",
      "source": "for epoch in range(3):\n    for i, (images, labels) in enumerate(train_loader):\n        images = Variable(images)\n        labels = Variable(labels)\n\n        optimizer.zero_grad()\n        outputs = cnn(images)\n\n        loss = loss_fn(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        if (i+1) % 100 == 0:\n            print ('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f' %(epoch+1, 5, i+1, len(train_data)//50,  loss.data[0]))\n",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Epoch [1/5], Iter [100/840] Loss: 2.0436\nEpoch [1/5], Iter [200/840] Loss: 1.7354\nEpoch [1/5], Iter [300/840] Loss: 1.5608\nEpoch [1/5], Iter [400/840] Loss: 1.2858\nEpoch [1/5], Iter [500/840] Loss: 1.0737\nEpoch [1/5], Iter [600/840] Loss: 1.1584\nEpoch [1/5], Iter [700/840] Loss: 0.9174\nEpoch [1/5], Iter [800/840] Loss: 0.8727\nEpoch [2/5], Iter [100/840] Loss: 0.8155\nEpoch [2/5], Iter [200/840] Loss: 0.8093\nEpoch [2/5], Iter [300/840] Loss: 0.8180\nEpoch [2/5], Iter [400/840] Loss: 0.5827\nEpoch [2/5], Iter [500/840] Loss: 0.5634\nEpoch [2/5], Iter [600/840] Loss: 0.6002\nEpoch [2/5], Iter [700/840] Loss: 0.5440\nEpoch [2/5], Iter [800/840] Loss: 0.5681\nEpoch [3/5], Iter [100/840] Loss: 0.5709\nEpoch [3/5], Iter [200/840] Loss: 0.6514\nEpoch [3/5], Iter [300/840] Loss: 0.4700\nEpoch [3/5], Iter [400/840] Loss: 0.4380\nEpoch [3/5], Iter [500/840] Loss: 0.5014\nEpoch [3/5], Iter [600/840] Loss: 0.5057\nEpoch [3/5], Iter [700/840] Loss: 0.5604\nEpoch [3/5], Iter [800/840] Loss: 0.4555\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "3a0306eeaaa5277c0bf9b60f6a64c38989335736"
      },
      "cell_type": "markdown",
      "source": "So ,finally we see that in just 3 epochs the loss value has come down to 0.45 from 2.04 . "
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "e0ed063552f8712aff78cfa4a53d9d9395c6de8d"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}