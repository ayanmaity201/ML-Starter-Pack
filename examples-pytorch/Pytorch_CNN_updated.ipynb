{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "75651683-fb32-43cd-8b34-8eb87e297ca8",
        "_uuid": "2430cad589e13a115f77d5963d35b4a07d1548e6"
      },
      "cell_type": "markdown",
      "source": "# Pytorch Convolution Neural Network "
    },
    {
      "metadata": {
        "_cell_guid": "d9a6e318-a004-4363-9d1b-8fd7978bd143",
        "_uuid": "8d8dca65653c45ed75e1eb89aa6de7045709735d"
      },
      "cell_type": "markdown",
      "source": "In this notebook I am going to build a convolution neural network using torch.nn module. We are going to train the CNN on MNIST data set . "
    },
    {
      "metadata": {
        "_cell_guid": "b98bf7d9-7676-49df-b59f-05466d666c16",
        "_uuid": "991df7918a2b37a1e9a0fd183f5e393daac82772",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "test.csv\ntrain.csv\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "d59dacd4-0500-4e74-9ac7-47c6f16b5fc3",
        "_uuid": "2e34fb48f1d3bbe097d8a31f7f92de16bf9a7565"
      },
      "cell_type": "markdown",
      "source": "Here first we have to import several torch libraries like torch.nn , torch.data.utils, torchvision etc."
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "fbed93cc20292ec5af65417659342f3de3e67115"
      },
      "cell_type": "code",
      "source": "df = pd.read_csv(\"../input/train.csv\")",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "bf88f044-e2d7-411d-907e-232e4d889ab5",
        "_uuid": "096b7ce8f555205e2f8336d980dab1389234afb9",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "import torch\nimport torch.nn as nn\nimport torch.utils.data as data\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "41d9f20a-6178-4009-a4ce-76b30255c211",
        "_uuid": "d9d57561bf4633b780cc3ca5e63143b2c25eaa4a"
      },
      "cell_type": "markdown",
      "source": "A very important part in building any deep NN in pytorch is first building a class conating the data . This data.datatset class wil contain  init() , getitem() , len() . After defining the class we create an object of that class . Then call the dataloader on the data with    batch_size = 50 .  "
    },
    {
      "metadata": {
        "_cell_guid": "384fdfca-4bc2-474f-8c76-37a183352065",
        "_uuid": "397c8f7f9065d95163c89ef9ea256737a2a6fb4a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "class Mnist(data.Dataset):\n    def __init__(self):\n        train_X = df[0:20000]\n        train_Y = train_X.label.as_matrix().tolist()\n        train_X = train_X.drop(\"label\",axis=1).as_matrix().reshape(20000,1,28,28)\n        self.datalist = train_X\n        self.labellist = train_Y\n\n\n    def __getitem__(self, index):\n        return torch.Tensor(self.datalist[index].astype(float)), self.labellist[index]\n\n    def __len__(self):\n        return self.datalist.shape[0]\n\ntrain_data = Mnist()\ntrain_loader = torch.utils.data.DataLoader(dataset=train_data,\n                                           batch_size=50, \n                                           shuffle=True,\n                                           num_workers=2)",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "437fe13c-45c7-4d21-b7bf-10400f499b46",
        "_uuid": "4773ad2c658e56de2d57d406d63eb01830e494c8"
      },
      "cell_type": "markdown",
      "source": "Now we create the nn.module class CNN . It must contain two methods init ( ) and forward ( ) . In init ( ) we define the sequential steps of the CNN model using Conv2d , ReLU , MaxPool2d etc. In forward method we do the forward propagation part . "
    },
    {
      "metadata": {
        "_cell_guid": "96371adb-fef6-4a6a-9b69-9e124b109b90",
        "_uuid": "e33cb98156db0af64faa493efb80ed86935c092e",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "class CNN(nn.Module):\n    def __init__(self):\n        super(CNN,self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(1, 20, kernel_size=5),\n            nn.BatchNorm2d(20),\n            nn.ReLU(),\n            nn.MaxPool2d(2))\n        self.fc = nn.Linear(12*12*20, 10)\n\n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = out.view(out.size(0),-1)\n        out = self.fc(out)\n        return out\n    \ncnn = CNN()",
      "execution_count": 25,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "c808abba-a060-4c85-8df5-e0ffe9f46913",
        "_uuid": "512c275630a5680527ce15e65538b28f483664a1"
      },
      "cell_type": "markdown",
      "source": "After craeting an instance cnn of class CNN now we define the loss function and the optimizer technique , which is SGD here with learning rate = .004"
    },
    {
      "metadata": {
        "_cell_guid": "6471fe1b-26b0-43d5-bf81-b47f0936355f",
        "_uuid": "f004a344b54575c0072f7e92b2cfd5646edd8268",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "loss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(cnn.parameters(), lr=0.001)",
      "execution_count": 26,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "ee08fb9c-5c5b-45e9-b2a5-ad25ce64382c",
        "_uuid": "8d5f06dc73538e1c055538583b47e993e9fa6bff"
      },
      "cell_type": "markdown",
      "source": "Here we start the actual CNN implemetation through iterating over allthe examples . Here we are only running 5 iterations and in each epoch we first call cnn on each image then we do the backward propagation and update the parameters . "
    },
    {
      "metadata": {
        "_cell_guid": "3f7a70d7-e137-4dd0-bdef-e19296e772e0",
        "_uuid": "80ebd54a4c6f68c5017a4c2cedfbe7e72af9f0cf",
        "trusted": true
      },
      "cell_type": "code",
      "source": "for epoch in range(5):\n    for i, (images, labels) in enumerate(train_loader):\n        images = Variable(images)\n        labels = Variable(labels)\n\n        optimizer.zero_grad()\n        outputs = cnn(images)\n\n        loss = loss_fn(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        if (i+1) % 100 == 0:\n            print ('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f' %(epoch+1, 5, i+1, len(train_data)//50,  loss.data[0]))\n",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Epoch [1/5], Iter [100/400] Loss: 1.6960\nEpoch [1/5], Iter [200/400] Loss: 1.1452\nEpoch [1/5], Iter [300/400] Loss: 0.8889\nEpoch [1/5], Iter [400/400] Loss: 0.8767\nEpoch [2/5], Iter [100/400] Loss: 0.5645\nEpoch [2/5], Iter [200/400] Loss: 0.5229\nEpoch [2/5], Iter [300/400] Loss: 0.7331\nEpoch [2/5], Iter [400/400] Loss: 0.4370\nEpoch [3/5], Iter [100/400] Loss: 0.4719\nEpoch [3/5], Iter [200/400] Loss: 0.5650\nEpoch [3/5], Iter [300/400] Loss: 0.2443\nEpoch [3/5], Iter [400/400] Loss: 0.4523\nEpoch [4/5], Iter [100/400] Loss: 0.4255\nEpoch [4/5], Iter [200/400] Loss: 0.3857\nEpoch [4/5], Iter [300/400] Loss: 0.4340\nEpoch [4/5], Iter [400/400] Loss: 0.3303\nEpoch [5/5], Iter [100/400] Loss: 0.2812\nEpoch [5/5], Iter [200/400] Loss: 0.3963\nEpoch [5/5], Iter [300/400] Loss: 0.3730\nEpoch [5/5], Iter [400/400] Loss: 0.2911\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "7f6321da-605f-4b2a-8b00-a0a830ae92c1",
        "_uuid": "d3610ef4a859283f28d77fc6bf4a1ed1401cbd37"
      },
      "cell_type": "markdown",
      "source": "So ,finally we see that in just 3 epochs the loss value has come down to 0.45 from 2.04 . "
    },
    {
      "metadata": {
        "_uuid": "84436faef68df30ed0bff3e473d4f9c1dd1b318e"
      },
      "cell_type": "markdown",
      "source": "Now we will be create a test data and apply our cnn on that."
    },
    {
      "metadata": {
        "_cell_guid": "8d9bb530-b847-4fe4-8e52-de82c4845e6d",
        "_uuid": "15408be9ab8517fb3494adbe9b0b312d84472ef0",
        "trusted": true
      },
      "cell_type": "code",
      "source": "testX = df[20000:22000]\ntestY = testX['label'].values\ntestX = testX.drop('label',axis=1).as_matrix().reshape(2000,1,28,28).astype(float)",
      "execution_count": 28,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "ef3d53088d8815d02d682246889edc0b3e00aca1"
      },
      "cell_type": "code",
      "source": "testX = Variable(torch.Tensor(testX))\npred = cnn(testX)\n_, predlabel = torch.max(pred.data, 1)",
      "execution_count": 29,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a6f372187db7759acaa84c350ad871def3ba237c"
      },
      "cell_type": "markdown",
      "source": "We run the cnn on the test data and find accuarcy of  90.7%"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e21c8f11c50c1cc955d3cf4ca904b2dd2cffac89"
      },
      "cell_type": "code",
      "source": "np.sum(predlabel.numpy()==testY)/2000",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 30,
          "data": {
            "text/plain": "0.90700000000000003"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "31712f2feb31e49ce26da2abc5bb9ba50f54134f"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}